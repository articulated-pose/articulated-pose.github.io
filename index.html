<!DOCTYPE HTML>
<html>
	<head>
		<title>Category-level </title>
		<meta charset="utf-8" />
		<!-- <meta name="viewport" content="width=device-width, initial-scale=1.0" /> -->
        <meta name="viewport" content="width=1000">
		<!--[if lte IE 8]><script src="assets/js/ie/html5shiv.js"></script><![endif]-->
		<link rel="stylesheet" href="assets/css/main.css" />
		<!--[if lte IE 8]><link rel="stylesheet" href="assets/css/ie8.css" /><![endif]-->
        <script>
          (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
          (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
          m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
          })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
          ga('create', 'UA-89797207-1', 'auto');
          ga('send', 'pageview');
        </script>

      <meta property="og:url"           content="https://articulated-pose.github.io/" />
	    <meta property="og:type"          content="website" />
	    <meta property="og:title"         content="Category-Level Articulated Object Pose Estimation" />
	    <meta property="og:description"   content="This paper addresses the task of category-level pose
																								estimation for articulated objects from a single depth image.
																								We present a novel category-level approach that correctly
																								accommodates object instances not previously seen during
																								training. A key aspect of the work is the new Articulation-
																								Aware Normalized Coordinate Space Hierarchy (A-NCSH),
																								which represents the different articulated objects for a
																								given object category. This approach not only provides
																								the canonical representation of each rigid part, but also
																								normalizes the joint parameters and joint states. We
																								developed a deep network based on PointNet++ that is
																								capable of predicting an A-NCSH representation for unseen
																								object instances from single depth input. The predicted
																								A-NCSH representation is then used for global pose
																								optimization using kinematic constraints. We demonstrate
																								that constraints associated with joints in the kinematic
																								chain lead to improved performance in estimating pose
																								and relative scale for each part of the object. We also
																								demonstrate that the approach can tolerate cases of severe
																								occlusion in the observed data. Code and data will be
																								publicly available." />
	    <meta property="og:image" content="http://tossingbot.cs.princeton.edu/images/teaser.png" />

	</head>
	<body id="top">

		<!-- Main -->
			<div id="main" style="padding-bottom:1em; padding-top: 5em; width: 60em; max-width: 70em; margin-left: auto; margin-right: auto;">
					<section id="four">

						<!-- <p><ul class="icons"><li><a href="#" class="icon fa-wrench"><span class="label">Twitter</span>&nbsp;&nbsp;This page is still under construction.</a></li></ul></p> -->

						<h1 style="text-align: center; margin-bottom: 0;"><font color="4e79a7">Category-Level Articulated Object Pose Estimation</font></h1>
						<!-- <h2 style="text-align: center;">with Residual Physics</h2> -->
						<span class="image right" style="max-width: 40%; margin-top: 0.5em; margin-bottom: 0; border: 2px solid #415161;"><img src="images/teaser.png" alt="" /></span>

						<p>Our environment is populated with articulated objects, ranging from furniture such as cabinets or ovens to small tabletop objects such as laptops or eyeglasses. Effectively interacting with these objects
						requires a detailed understanding of their articulation states and part-level poses. Such understanding is beyond the scope of typical 6D pose estimation algorithms, which have been designed for rigid objects
						Algorithms that do consider object articulations often require the exact object CAD model and the associated joint parameters at test time, preventing them from generalizing to new object instances.

						<br><br>This paper addresses the task of category-level pose estimation for articulated objects from a single depth image. We present a novel category-level approach that correctly accommodates object instances not previously seen  during training.
						A key aspect of the work is the new Articulation-Aware Normalized Coordinate SpaceHierarchy (A-NCSH), which represents the different articulated objects for a given object category.
						This approach not only provides the  canonical representation of each rigid part, but also normalizes the joint parameters and joint states.
						We developed a deep network based on PointNet++ that is capable of predicting an A-NCSH  representation   for   unseen  object instances from single depth input. The predicted A-NCSH  representation is then used for global pose optimization using kinematic constraints.
						We demonstrate that constraints associated with joints in the kinematic chain lead to improved performance in estimating pose and relative scale for each part of the object.
						We also demonstrate that the approach can tolerate cases of severe occlusion in the observed data.
						<!-- <iframe id="match-video" width="853" height="480" style="margin-bottom: 2em; margin-left: auto; margin-right: auto; display:block;" src="https://www.youtube.com/embed/-O-E1nFm6-A?rel=0" frameborder="0" allowfullscreen></iframe> -->
						<!-- <hr/ style="margin-top: 1em"> -->
						<div id="files" class="center" style="margin-top: 15px">
							[ <a href="https://arxiv.org/abs/1912.11913">Paper</a> ]&nbsp;&nbsp;&nbsp;&nbsp;
							[ <a href="https://articulated-pose.github.io/">Code</a> ]&nbsp;&nbsp;&nbsp;&nbsp;
							[ <a href="https://articulated-pose.github.io/">Pretrained Model</a> ]&nbsp;&nbsp;&nbsp;&nbsp;
							[ <a href="https://articulated-pose.github.io/">Data</a> ]

						</div>
						<hr style="margin-top: 3em;">
						<h3>Results</h3>
						<h4>Synthetic Dataset: <font color="4e2997">continuous articulation, fixed view point</font></h4>
						<!-- <span class="center"><img src="images/gifs/gt_pred_0003.gif" width="1000"><img src="images/gifs/gt_pred_46123.gif" width="1000"></span> -->
						<span class="center"><img src="images/gifs/pred_proj_final.gif" width="100%"></span>
						<h4>Synthetic Dataset: <font color="4e2997">random articulation, random view point</font></h4>
						<span class="center"><img src="images/gifs/pred_proj_syn.gif" width="100%"></span>
						<h4>Real Dataset: <font color="4e2997">simu-to-real, instance-level</font></h4>
						<span class="center"><img src="images/results/real_result1.png" width="100%"></span>
						<hr style="margin-top: 3em;">
						<h3>Paper</h3>

						<!-- <hr/> -->
						<p style="margin-bottom: 1em;">Latest version (Dec 26, 2019): <a href="https://arxiv.org/abs/1912.11913">arXiv:1912.11913 in cs.CV</a> or <a href="paper.pdf">here</a>.<!-- <br>To appear at IEEE International Conference on Intelligent Robots and Systems (IROS) 2018<br><font color="4e79a7">&#9733; Best Cognitive Robotics Paper Award Finalist, IROS &#9733;</font> --></p>

						<div class="12u$"><a href="https://arxiv.org/abs/1912.11913"><span class="image fit" style="border: 1px solid; border-color: #888888;"><img src="images/paper-thumbnail.png" alt="" /></span></a></div>

						<hr>
						<h3>Team</h3>

						<section>
							<div class="box alt" style="margin-bottom: 1em;">
								<div class="row 50% uniform" style="width: 80%;">
									<div class="2u" style="font-size: 0.7em; line-height: 1.5em; text-align: center;"><a href="http://dragonlong.github.io/"><span class="image fit" style="margin-bottom: 0.5em;"><img src="images/xiaolong-thumbnail.png" alt="" style="border-radius: 50%;" /></span>Xiaolong Li <sup>1</sup></a></div>
									<div class="2u" style="font-size: 0.7em; line-height: 1.5em; text-align: center;"><a href="http://ai.stanford.edu/~hewang/"><span class="image fit" style="margin-bottom: 0.5em;"><img src="images/hewang-thumbnail.jpg" alt="" style="border-radius: 50%;" /></span> He Wang <sup>2</sup></a></div>
									<div class="2u" style="font-size: 0.7em; line-height: 1.5em; text-align: center;"><a href="https://cs.stanford.edu/~ericyi/"><span class="image fit" style="margin-bottom: 0.5em;"><img src="images/ericyi-thumbnail.jpg" alt="" style="border-radius: 50%;" /></span> Li YI <sup>2</sup></a></div>
									<div class="2u" style="font-size: 0.7em; line-height: 1.5em; text-align: center;"><a href="https://geometry.stanford.edu/member/guibas/index.html"><span class="image fit" style="margin-bottom: 0.5em;"><img src="images/guibas-thumbnail.png" alt="" style="border-radius: 50%;" /></span> Leonidas J. Guibas <sup>2</sup></a></div>
									<div class="2u" style="font-size: 0.7em; line-height: 1.5em; text-align: center;"><a href="https://ece.vt.edu/people/profile/abbott"><span class="image fit" style="margin-bottom: 0.5em;"><img src="images/abbott-thumbnail.png" alt="" style="border-radius: 50%;" /></span> A. Lynn Abbott <sup>1</sup></a></div>
									<div class="2u" style="font-size: 0.7em; line-height: 1.5em; text-align: center;"><a href="http://vision.princeton.edu/people/shurans/"><span class="image fit" style="margin-bottom: 0.5em;"><img src="images/shuran-thumbnail.jpg" alt="" style="border-radius: 50%;" /></span>Shuran Song <sup>3</sup></a></div>
									<!-- <div class="2u"><span class="image fit"><img src="images/thumbs/02.jpg" alt="" /></span></div>
									<div class="2u"><span class="image fit"><img src="images/thumbs/03.jpg" alt="" /></span></div>
									<div class="2u"><span class="image fit"><img src="images/thumbs/04.jpg" alt="" /></span></div>
									<div class="2u"><span class="image fit"><img src="images/thumbs/05.jpg" alt="" /></span></div>
									<div class="2u$"><span class="image fit"><img src="images/thumbs/06.jpg" alt="" /></span></div> -->
								</div>
							</div>
						</section>
						<sup>1</sup> Virginia Tech&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<sup>2</sup> Stanford University&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<sup>3</sup> Columbia University&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
						<div class="row" style="margin-top: 1em">
							<div class="12u$ 1u$(xsmall)">
								<h3>Bibtex</h3>
								<pre style="text-align:left;"><code>@article{li2019articulated-pose, 
title={Category-Level Articulated Object Pose Estimation},
author={Li, Xiaolong and Wang, He and Yi, Li and Guibas, Leonidas and Abbott, A. Lynn and Song, Shuran},
journal={arXiv preprint  arXiv:1912.11913},
year={2019} }
							</code></pre>
							</div>
						</div>
						<hr/ style="margin-top: 1em">


						<!-- <div class="row">
							<div class="12u$ 12u$(xsmall)" style="text-align: center;">
								<h3>Introductory Video (with audio)</h3>
								<iframe id="match-video" width="640" height="360" style="margin-bottom: 2em; margin-left: auto; margin-right: auto; display:block;" src="https://www.youtube.com/embed/-O-E1nFm6-A?rel=0" frameborder="0" allowfullscreen></iframe>
							</div>
						</div>
						<div class="row">
							<div class="12u$ 12u$(xsmall)" style="text-align: center;">
								<h3>Technical Summary Video (with audio)</h3>
								<iframe id="match-video" width="640" height="360" style="margin-bottom: 2em; margin-left: auto; margin-right: auto; display:block;" src="https://www.youtube.com/embed/f5Zn2Up2RjQ?rel=0" frameborder="0" allowfullscreen></iframe>
							</div>
						</div> -->

						<!-- <hr/ style="margin-top: 1em"> -->
				        <h3>Acknowledgements</h3>
								<p>This research was supported by a grant from Toyota-Stanford Center for AI Research. This research used resources provided by Advanced Research Computing within the Division of Information Technology at Virginia Tech. We thank Vision and Learning Lab at Virginia Tech for help on visualization tools. We are also grateful for financial and hardware support from Google. </p>
						<hr/ style="margin-top: 1em">
				        <h3>Contact</h3>
				        <p>If you have any questions, please feel free to contact <a href="http://dragonlong.github.io/">Xiaolong Li</a> at lxiaol9_at_vt.edu</p>
						<hr/>


						<!-- <div class="row">
							<div class="6u 12u$(xsmall)">
								<p>Sunday, Dec. 22nd, 2019<br>Posted by <a href="http://dragonlong.github.io/">Xiaolong Li</a></p>
							</div>
							</div>
						</div> -->
					</section>
			</div>

		<!-- Footer -->
			<footer id="footer">
				<div class="inner">
					<ul class="copyright">
						<li>Meet <a href="https://en.wikipedia.org/wiki/Danbo_(character)">Danbo</a> the cardboard robot.</li>
					</ul>
				</div>
			</footer>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.poptrox.min.js"></script>
			<script src="assets/js/skel.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>
	</body>
</html>
